{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce22a99-7056-4ab6-96f9-d84f406b8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "snp_data = pd.read_csv('output.csv')\n",
    "train_data = pd.read_csv('STR_train.csv',sep=';')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d61f2-7f18-4065-95ed-10746035f3f8",
   "metadata": {},
   "source": [
    "<font color='green'>Data Preprocessing.</font> SNP из FinalReport.csv и длины STR из STR_train.csv объединяются таким образом, что каждая строка в наших данных имеет один уникальный animal_id, 24 длины STR, и все уникальны SNP в FinalReport.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c3e273-e007-4375-8138-c0725abbb697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_ids = snp_data['animal_id'].unique()\n",
    "all_snp = snp_data['SNP Name'].unique()\n",
    "all_str = train_data['STR Name'].unique()\n",
    "\n",
    "x=[]\n",
    "x = np.insert(all_snp,0,'animal_id')\n",
    "for str_n in all_str:\n",
    "    x = np.append(x, str_n+'_allele1')\n",
    "    x = np.append(x, str_n+'_allele2')\n",
    "    \n",
    "finaldf = pd.DataFrame(x)\n",
    "finaldf = finaldf.T\n",
    "finaldf.columns = finaldf.iloc[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b985795c-6ab5-4e5d-b7ce-65b1b4f1cdc9",
   "metadata": {},
   "source": [
    "Данные SNP из FinalReport.csv кодируются по следующей схеме: <br>\n",
    "AA = 0\n",
    "AB = 1\n",
    "BB = 2\n",
    "</br>\n",
    "При отсутствии данных они заполняются с «reference» или 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5744b1d-d7c3-4ead-9cc4-18609f539791",
   "metadata": {},
   "outputs": [],
   "source": [
    "gozi = finaldf\n",
    "rowIndex = gozi.index[0]\n",
    "for animal_id in all_ids:\n",
    "    rowsnp = snp_data.loc[snp_data['animal_id'] == animal_id]\n",
    "    rowstr = train_data.loc[train_data['animal_id'] == animal_id]\n",
    "    rowIndex = rowIndex + 1\n",
    "    gozi.loc[rowIndex, 'animal_id'] = animal_id\n",
    "    for row in rowsnp.itertuples():\n",
    "        if row._6=='A' and row._7=='A':\n",
    "            gozi.at[rowIndex, str(row._3)] = 0\n",
    "        elif (row._6=='A' and row._7=='B') or (row._6=='B' and row._7=='A') :\n",
    "            gozi.at[rowIndex, str(row._3)] = 1\n",
    "        elif row._6=='B' and row._7=='B':\n",
    "            gozi.at[rowIndex, str(row._3)] = 2\n",
    "        else:\n",
    "            gozi.at[rowIndex, str(row._3)] = 0\n",
    "    for row in rowstr.itertuples():\n",
    "        gozi.at[rowIndex, str(row._2)+'_allele1'] = row.Allele1\n",
    "        gozi.at[rowIndex, str(row._2)+'_allele2'] = row.Allele2\n",
    "#gozi.to_csv('gozi_missing_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c3ee2-123f-4073-ac44-67472e78177c",
   "metadata": {},
   "source": [
    "Есть ~535 animal_ids, для которых отсутствует большая часть длины ST row index 1967-2501. Эти строки удалены из наших данных. Есть около 100 столбцов между index 2887 и 2987, для которых более половины animal_ids не имеют данных. Эти столбцы были удалены, как и 3 других столбца (BovineHD0200....), для которых также отсутствует более половины записей. Все они удалены из наших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb84d507-8ce6-4cd8-b5ef-1c02e0e83707",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('gozi_missing_0.csv',low_memory=False)\n",
    "file = file.drop(gzv.iloc[:, 2887:2987], axis=1)\n",
    "file = file.drop(gzz.index[1967:2501])\n",
    "file.drop(labels='BovineHD0200037029',axis=1,inplace=True)\n",
    "file.drop(labels='BovineHD0300010318',axis=1,inplace=True)\n",
    "file.drop(labels='BovineHD0300010328',axis=1,inplace=True)\n",
    "file.to_csv('no_missing_snps.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc526d-d44e-4ea4-ad0c-4aba7fea4e34",
   "metadata": {},
   "source": [
    " пропущенные значения длины STR аллеля были заполнены медианным значением этого столбца. Я сделал это в Excel. <https://drive.google.com/file/d/1BAGedFXgS95m-pbM_D7AYyXnojPxX24n/view?usp=sharing> - это входной файл, используемый в следующем разделе.\n",
    "\n",
    "Read data, and separate them into X (inputs) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10683093-5b6c-4e7c-98e0-d1191309ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('snp_0_str_median.csv',low_memory=False)\n",
    "x_data = all_data.iloc[:, :2883] # First 2880 columns are SNPs\n",
    "y_data = all_data.iloc[:, -24:]  # Remaining columns are STR lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535aef31-1d34-422b-b224-625ab335cfd1",
   "metadata": {},
   "source": [
    "Поскольку наши данные в высокой степени «категоричны» — значения данных SNP могут принимать только дискретные значения, которые представляют категории. Кроме того, для каждого конкретного STR длина принимает значения в небольшом диапазоне. Для таких данных два основных варианта моделей ИИ — это либо нейронная сеть со слоем Embedding, либо модель градиентного усиления. Модели градиентного усиления, такие как XGBoost, показали очень хорошую производительность для высокоразмерных категориальных данных. Я пробовал использовать эти данные в глубокой нейронной сети с внедрением. Модель XGBoost дает лучшие результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7c4acd-b5fb-44f4-841f-e00fe54451ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\az\\envs\\tf\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [06:09:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"early_stopping_rounds\", \"num_boost_round\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:36.85756\ttest-rmse:37.31440\n",
      "[23]\ttrain-rmse:1.02799\ttest-rmse:6.99556\n",
      "Mean Squared Error: 48.95500730545925\n",
      "R-squared Score: 0.45429691672325134\n",
      "Top 10 most important features:\n",
      "f63: 51586.484375\n",
      "f2235: 26947.2734375\n",
      "f1706: 25844.921875\n",
      "f1998: 20436.478515625\n",
      "f1593: 19988.34375\n",
      "f1713: 16088.919921875\n",
      "f1792: 14964.7666015625\n",
      "f2242: 12096.85546875\n",
      "f2217: 11740.6435546875\n",
      "f1788: 11443.03125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "X = x_data  \n",
    "y = y_data \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'num_boost_round': 100,\n",
    "    'early_stopping_rounds': 10,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(params, dtrain, num_boost_round=1000,\n",
    "                  evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "                  early_stopping_rounds=10, verbose_eval=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(dtest)\n",
    "pd.DataFrame(y_pred).to_csv('pred.csv')\n",
    "X_test.to_csv('xt.csv')\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")\n",
    "\n",
    "# Feature importance\n",
    "importance = model.get_score(importance_type='gain')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 most important features:\")\n",
    "for feature, score in importance[:10]:\n",
    "    print(f\"{feature}: {score}\")\n",
    "\n",
    "# Save the model\n",
    "#model.save_model('str_length_prediction_model.json')\n",
    "\n",
    "#print(\"Model saved as 'str_length_prediction_model.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de33e01-cf50-4de5-b05f-145fec3fa499",
   "metadata": {},
   "source": [
    "Как мы видим, модель XGBoost может быть использована для поиска признаков (SNP), которые вносят наибольший вклад в правильные прогнозы. План состоял в том, чтобы использовать 300 лучших признаков для создания новой модели, которая просто использует эти 300 признаков в качестве входных данных. Но затем я понял, что это не та задача\n",
    "\n",
    "создаем файл с SNP и их прогнозируемыми длинами STR. Его можно посмотреть здесь: <https://drive.google.com/file/d/1iA3gzxcAGKxPgf-WH2KmqL1dUEwV5-9i/view?usp=sharing>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed2fbc9-9855-4543-9119-e7a5a505c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file =  np.concatenate( (X_test, y_pred), axis=1 ) \n",
    "pd.DataFrame(g).to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a225da-353f-4f1f-a113-12ba00d4b205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
